{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290b67e6-a792-4cb7-a402-b86c48e5170d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def generate_combinations(n):\n",
    "    return np.array(list(itertools.product([0, 1], repeat=n)))\n",
    "\n",
    "def forward(X, w1, b1, w2, b2, w3, b3):\n",
    "    z1 = np.dot(X, w1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "\n",
    "    z2 = np.dot(a1, w2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    z3 = np.dot(a2, w3) + b3\n",
    "    out = sigmoid(z3)\n",
    "\n",
    "    return a1, a2, out\n",
    "\n",
    "# --- Start of main program ---\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "num_inputs = int(input(\"Enter the number of binary inputs: \"))\n",
    "X = generate_combinations(num_inputs)\n",
    "\n",
    "# Desired outputs\n",
    "y = []\n",
    "for row in X:\n",
    "    print(f\"Desired output for {list(row)}: \")\n",
    "    out = float(input(\"Output (0 or 1): \"))\n",
    "    y.append(out)\n",
    "y = np.array(y).reshape(-1, 1)\n",
    "\n",
    "# Initialize weights and biases randomly\n",
    "hidden1_size = 4\n",
    "hidden2_size = 3\n",
    "output_size = 1\n",
    "\n",
    "w1 = np.random.randn(num_inputs, hidden1_size)\n",
    "b1 = np.zeros((1, hidden1_size))\n",
    "\n",
    "w2 = np.random.randn(hidden1_size, hidden2_size)\n",
    "b2 = np.zeros((1, hidden2_size))\n",
    "\n",
    "w3 = np.random.randn(hidden2_size, output_size)\n",
    "b3 = np.zeros((1, output_size))\n",
    "\n",
    "# Training parameters\n",
    "epochs = 10000\n",
    "learning_rate = 0.1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    a1, a2, output = forward(X, w1, b1, w2, b2, w3, b3)\n",
    "\n",
    "    # Compute loss (optional print)\n",
    "    loss = np.mean((y - output) ** 2)\n",
    "\n",
    "    # Backward pass\n",
    "    error = y - output\n",
    "    d_output = error * sigmoid_derivative(output)\n",
    "\n",
    "    error_hidden2 = d_output.dot(w3.T)\n",
    "    d_hidden2 = error_hidden2 * sigmoid_derivative(a2)\n",
    "\n",
    "    error_hidden1 = d_hidden2.dot(w2.T)\n",
    "    d_hidden1 = error_hidden1 * sigmoid_derivative(a1)\n",
    "\n",
    "    # Update weights and biases\n",
    "    w3 += a2.T.dot(d_output) * learning_rate\n",
    "    b3 += np.sum(d_output, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    w2 += a1.T.dot(d_hidden2) * learning_rate\n",
    "    b2 += np.sum(d_hidden2, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    w1 += X.T.dot(d_hidden1) * learning_rate\n",
    "    b1 += np.sum(d_hidden1, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    if (epoch+1) % 1000 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(\"Final Weights and Biases:\")\n",
    "print(\"w1:\", w1)\n",
    "print(\"b1:\", b1)\n",
    "print(\"w2:\", w2)\n",
    "print(\"b2:\", b2)\n",
    "print(\"w3:\", w3)\n",
    "print(\"b3:\", b3)\n",
    "\n",
    "# Final predictions\n",
    "_, _, final_output = forward(X, w1, b1, w2, b2, w3, b3)\n",
    "print(\"\\nFinal predictions (rounded):\")\n",
    "print(np.round(final_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a122280-bb75-4e82-aa6f-d98f715a7822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import itertools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f29c37cd-2088-4e65-81a3-74f7660eff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_combination(n):\n",
    "    return np.array(list(itertools.product([0, 1], repeat=n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcc697cf-1ebb-465a-9909-be4bfe923741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return (1/(1 + np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4688091-6c27-484a-bb46-a720511be403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c92c4b32-d709-40c7-8b3e-a7ca7b1bd300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_chaining(X, w1, b1, w2, b2, w3, b3):\n",
    "\n",
    "    z1 = np.dot(X, w1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "\n",
    "    z2 = np.dot(a1, w2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    z3 = np.dot(a2, w3) + b3\n",
    "    a3 = sigmoid(z3)\n",
    "\n",
    "    return a1, a2, a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68eeca31-7bd4-45a5-b6ba-c647587e1d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number of binary inputs:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the output for the inputs [0, 0]: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the output for the inputs [0, 1]: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the output for the inputs [1, 0]: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the output for the inputs [1, 1]: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 0\n"
     ]
    }
   ],
   "source": [
    "num_inputs = int(input(\"Enter the number of binary inputs: \"))\n",
    "X = generate_combination(num_inputs)\n",
    "\n",
    "\n",
    "output = []\n",
    "for row in X:\n",
    "    print(f\"Enter the output for the inputs {list(row)}: \")\n",
    "    a = int(input())\n",
    "    output.append(a)\n",
    "\n",
    "y = np.array(output).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b81bfac-5ad0-4761-befc-9e8d0d9a0624",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden1 = 4\n",
    "n_hidden2 = 3\n",
    "output_size = 1 \n",
    "\n",
    "epochs = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "w1 = np.random.randn(num_inputs, n_hidden1)\n",
    "b1 = np.zeros((1, n_hidden1))\n",
    "w2 = np.random.randn(n_hidden1, n_hidden2)\n",
    "b2 = np.zeros((1, n_hidden2))\n",
    "w3 = np.random.randn(n_hidden2, output_size)\n",
    "b3 = np.zeros((1, output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e070a242-2a29-480c-9e11-4f57f6f95f1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,4) (1,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs): \n\u001b[0;32m----> 2\u001b[0m     a1, a2, result \u001b[38;5;241m=\u001b[39m forward_chaining(X, w1, b1, w2, b2, w3, b3)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Loss\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean((y \u001b[38;5;241m-\u001b[39m result)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m, in \u001b[0;36mforward_chaining\u001b[0;34m(X, w1, b1, w2, b2, w3, b3)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_chaining\u001b[39m(X, w1, b1, w2, b2, w3, b3):\n\u001b[0;32m----> 3\u001b[0m     z1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X, w1) \u001b[38;5;241m+\u001b[39m b1\n\u001b[1;32m      4\u001b[0m     a1 \u001b[38;5;241m=\u001b[39m sigmoid(z1)\n\u001b[1;32m      6\u001b[0m     z2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(a1, w2) \u001b[38;5;241m+\u001b[39m b2\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,4) (1,3) "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs): \n",
    "    a1, a2, result = forward_chaining(X, w1, b1, w2, b2, w3, b3)\n",
    "\n",
    "    # Loss\n",
    "    loss = np.mean((y - result)**2)\n",
    "\n",
    "    # Back propagation \n",
    "    error = y - result\n",
    "    d_output  = error * sigmoid_derivative(result)\n",
    "    \n",
    "    error2 = d_output.dot(w3.T)\n",
    "    d_hidden2 = error2 * sigmoid_derivative(a2) \n",
    "\n",
    "    error1 = d_hidden2.dot(w2.T) \n",
    "    d_hidden1 = error1* sigmoid_derivative(a1) \n",
    "\n",
    "    w3 += a2.T.dot(d_output) * learning_rate\n",
    "    b3 += np.sum(d_output, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    w2 += a1.T.dot(d_hidden2) * learning_rate\n",
    "    b2 +=  np.sum(d_hidden2, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    w1 += X.T.dot(d_hidden1) * learning_rate\n",
    "    b1 += np.sum(d_hidden1, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    if (epoch+1) % 1000 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da063a82-244f-47aa-b003-ed01bc08260b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number of binary inputs:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the output for the inputs [0, 0]: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the output for the inputs [0, 1]: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the output for the inputs [1, 0]: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the output for the inputs [1, 1]: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 0.2505683717875393\n",
      "Epoch 200, Loss: 0.2504305904142883\n",
      "Epoch 300, Loss: 0.25030873588503755\n",
      "Epoch 400, Loss: 0.250198788054461\n",
      "Epoch 500, Loss: 0.2500979121809827\n",
      "Epoch 600, Loss: 0.25000372204150384\n",
      "Epoch 700, Loss: 0.24991415886008045\n",
      "Epoch 800, Loss: 0.24982739236992557\n",
      "Epoch 900, Loss: 0.24974173755146334\n",
      "Epoch 1000, Loss: 0.24965558172663077\n",
      "Epoch 1100, Loss: 0.2495673174445302\n",
      "Epoch 1200, Loss: 0.24947527700933045\n",
      "Epoch 1300, Loss: 0.2493776646006386\n",
      "Epoch 1400, Loss: 0.24927248171381405\n",
      "Epoch 1500, Loss: 0.2491574410672968\n",
      "Epoch 1600, Loss: 0.249029863115185\n",
      "Epoch 1700, Loss: 0.24888654775141716\n",
      "Epoch 1800, Loss: 0.24872361152730912\n",
      "Epoch 1900, Loss: 0.24853627748969476\n",
      "Epoch 2000, Loss: 0.2483186002700764\n",
      "Epoch 2100, Loss: 0.24806310293960912\n",
      "Epoch 2200, Loss: 0.24776029401538854\n",
      "Epoch 2300, Loss: 0.24739802268366817\n",
      "Epoch 2400, Loss: 0.2469606183111602\n",
      "Epoch 2500, Loss: 0.24642774898094028\n",
      "Epoch 2600, Loss: 0.2457729296570152\n",
      "Epoch 2700, Loss: 0.24496162911619704\n",
      "Epoch 2800, Loss: 0.2439489988038092\n",
      "Epoch 2900, Loss: 0.2426774390691653\n",
      "Epoch 3000, Loss: 0.24107463320776418\n",
      "Epoch 3100, Loss: 0.23905344956624625\n",
      "Epoch 3200, Loss: 0.23651628752634865\n",
      "Epoch 3300, Loss: 0.23336765760938877\n",
      "Epoch 3400, Loss: 0.22953862417565266\n",
      "Epoch 3500, Loss: 0.2250223973751833\n",
      "Epoch 3600, Loss: 0.21990972203056344\n",
      "Epoch 3700, Loss: 0.21440073586407543\n",
      "Epoch 3800, Loss: 0.20877240728087754\n",
      "Epoch 3900, Loss: 0.20330794538669078\n",
      "Epoch 4000, Loss: 0.1982256812056047\n",
      "Epoch 4100, Loss: 0.19364408142693135\n",
      "Epoch 4200, Loss: 0.18958843602446382\n",
      "Epoch 4300, Loss: 0.1860194449138497\n",
      "Epoch 4400, Loss: 0.18286256066308335\n",
      "Epoch 4500, Loss: 0.1800279729312938\n",
      "Epoch 4600, Loss: 0.1774201689366376\n",
      "Epoch 4700, Loss: 0.1749393386079398\n",
      "Epoch 4800, Loss: 0.17247666864688369\n",
      "Epoch 4900, Loss: 0.1699040866219304\n",
      "Epoch 5000, Loss: 0.16705726405935015\n",
      "Epoch 5100, Loss: 0.1637088749262767\n",
      "Epoch 5200, Loss: 0.15952747167467293\n",
      "Epoch 5300, Loss: 0.15401743754379707\n",
      "Epoch 5400, Loss: 0.1464449272855361\n",
      "Epoch 5500, Loss: 0.13580736885423372\n",
      "Epoch 5600, Loss: 0.1211008700103473\n",
      "Epoch 5700, Loss: 0.10241654572390392\n",
      "Epoch 5800, Loss: 0.08230835751372642\n",
      "Epoch 5900, Loss: 0.0644592965668298\n",
      "Epoch 6000, Loss: 0.050668829620084545\n",
      "Epoch 6100, Loss: 0.04064421732224771\n",
      "Epoch 6200, Loss: 0.03340931680747629\n",
      "Epoch 6300, Loss: 0.02809452118147726\n",
      "Epoch 6400, Loss: 0.024089825307490702\n",
      "Epoch 6500, Loss: 0.020994141271057913\n",
      "Epoch 6600, Loss: 0.01854503786529346\n",
      "Epoch 6700, Loss: 0.016567849820817722\n",
      "Epoch 6800, Loss: 0.01494348374986493\n",
      "Epoch 6900, Loss: 0.013588663673430743\n",
      "Epoch 7000, Loss: 0.012443747569763615\n",
      "Epoch 7100, Loss: 0.011465073913583125\n",
      "Epoch 7200, Loss: 0.010620042748412462\n",
      "Epoch 7300, Loss: 0.009883878806257775\n",
      "Epoch 7400, Loss: 0.009237452522720811\n",
      "Epoch 7500, Loss: 0.00866578169992664\n",
      "Epoch 7600, Loss: 0.00815698082723106\n",
      "Epoch 7700, Loss: 0.007701510975627072\n",
      "Epoch 7800, Loss: 0.007291635422495945\n",
      "Epoch 7900, Loss: 0.006921018610022789\n",
      "Epoch 8000, Loss: 0.006584426609937859\n",
      "Epoch 8100, Loss: 0.006277500562923542\n",
      "Epoch 8200, Loss: 0.005996583313474443\n",
      "Epoch 8300, Loss: 0.005738585321789003\n",
      "Epoch 8400, Loss: 0.005500879921639318\n",
      "Epoch 8500, Loss: 0.005281220746412303\n",
      "Epoch 8600, Loss: 0.005077676073001039\n",
      "Epoch 8700, Loss: 0.004888576200072312\n",
      "Epoch 8800, Loss: 0.004712470958214907\n",
      "Epoch 8900, Loss: 0.004548095161460045\n",
      "Epoch 9000, Loss: 0.004394340331875575\n",
      "Epoch 9100, Loss: 0.004250231415748822\n",
      "Epoch 9200, Loss: 0.004114907499058068\n",
      "Epoch 9300, Loss: 0.003987605748023354\n",
      "Epoch 9400, Loss: 0.0038676479663591443\n",
      "Epoch 9500, Loss: 0.003754429287927164\n",
      "Epoch 9600, Loss: 0.00364740862158381\n",
      "Epoch 9700, Loss: 0.003546100541268183\n",
      "Epoch 9800, Loss: 0.0034500683740408167\n",
      "Epoch 9900, Loss: 0.003358918285761179\n",
      "Epoch 10000, Loss: 0.0032722942013013088\n",
      "\n",
      "Final predictions:\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import itertools \n",
    "\n",
    "def generate_combination(n):\n",
    "    return np.array(list(itertools.product([0, 1], repeat=n)))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return (1/(1 + np.exp(-x)))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def forward_chaining(X, w1, b1, w2, b2, w3, b3):\n",
    "    z1 = np.dot(X, w1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "\n",
    "    z2 = np.dot(a1, w2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    z3 = np.dot(a2, w3) + b3\n",
    "    a3 = sigmoid(z3)\n",
    "\n",
    "    return a1, a2, a3\n",
    "\n",
    "num_inputs = int(input(\"Enter the number of binary inputs: \"))\n",
    "X = generate_combination(num_inputs)\n",
    "\n",
    "output = []\n",
    "for row in X:\n",
    "    print(f\"Enter the output for the inputs {list(row)}: \")\n",
    "    a = int(input())\n",
    "    output.append(a)\n",
    "\n",
    "y = np.array(output).reshape(-1, 1)\n",
    "\n",
    "n_hidden1 = 4\n",
    "n_hidden2 = 3\n",
    "output_size = 1 \n",
    "\n",
    "epochs = 10000\n",
    "learning_rate = 0.1\n",
    "\n",
    "w1 = np.random.randn(num_inputs, n_hidden1)\n",
    "b1 = np.zeros((1, n_hidden1))\n",
    "w2 = np.random.randn(n_hidden1, n_hidden2)\n",
    "b2 = np.zeros((1, n_hidden2))\n",
    "w3 = np.random.randn(n_hidden2, output_size)\n",
    "b3 = np.zeros((1, output_size))\n",
    "\n",
    "for epoch in range(epochs): \n",
    "    a1, a2, result = forward_chaining(X, w1, b1, w2, b2, w3, b3)\n",
    "\n",
    "    # Loss\n",
    "    loss = np.mean((y - result)**2)\n",
    "\n",
    "    # Back propagation \n",
    "    error = y - result\n",
    "    d_output  = error * sigmoid_derivative(result)\n",
    "    \n",
    "    error2 = d_output.dot(w3.T)\n",
    "    d_hidden2 = error2 * sigmoid_derivative(a2) \n",
    "\n",
    "    error1 = d_hidden2.dot(w2.T) \n",
    "    d_hidden1 = error1 * sigmoid_derivative(a1) \n",
    "\n",
    "    w3 += a2.T.dot(d_output) * learning_rate\n",
    "    b3 += np.sum(d_output, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    w2 += a1.T.dot(d_hidden2) * learning_rate\n",
    "    b2 += np.sum(d_hidden2, axis=0, keepdims=True) * learning_rate\n",
    "    \n",
    "    w1 += X.T.dot(d_hidden1) * learning_rate\n",
    "    b1 += np.sum(d_hidden1, axis=0, keepdims=True) * learning_rate  # Corrected here\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss}\")\n",
    "\n",
    "# Final Predictions\n",
    "print(\"\\nFinal predictions:\")\n",
    "_, _, final_output = forward_chaining(X, w1, b1, w2, b2, w3, b3)\n",
    "print(np.round(final_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9c6f2617-eb5b-4e52-b544-6203739b7f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number of binary inputs:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the output for the inputs [0, 0]: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the output for the inputs [0, 1]: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the output for the inputs [1, 0]: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the output for the inputs [1, 1]: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss: 0.32677961902337976\n",
      "epoch 100: loss: 0.2615583216249465\n",
      "epoch 200: loss: 0.25080844038027234\n",
      "epoch 300: loss: 0.23999783066097466\n",
      "epoch 400: loss: 0.22381594149326067\n",
      "epoch 500: loss: 0.19524735665406823\n",
      "epoch 600: loss: 0.14775891569712457\n",
      "epoch 700: loss: 0.09215183056344753\n",
      "epoch 800: loss: 0.052777341572530015\n",
      "epoch 900: loss: 0.031960575147567304\n",
      "epoch 1000: loss: 0.0212177119684567\n",
      "epoch 1100: loss: 0.015229389759387302\n",
      "epoch 1200: loss: 0.011591420488818885\n",
      "epoch 1300: loss: 0.009214928414761342\n",
      "epoch 1400: loss: 0.007570115923108632\n",
      "epoch 1500: loss: 0.00637836483206183\n",
      "epoch 1600: loss: 0.005482640325277647\n",
      "epoch 1700: loss: 0.004789072263929419\n",
      "epoch 1800: loss: 0.0042387041208142035\n",
      "epoch 1900: loss: 0.0037929317236891314\n",
      "epoch 2000: loss: 0.003425567307018761\n",
      "epoch 2100: loss: 0.0031182998439703152\n",
      "epoch 2200: loss: 0.002857986753900547\n",
      "epoch 2300: loss: 0.0026349779298864354\n",
      "epoch 2400: loss: 0.0024420447850017233\n",
      "epoch 2500: loss: 0.0022736762785625117\n",
      "epoch 2600: loss: 0.0021256044188562783\n",
      "epoch 2700: loss: 0.0019944772052630823\n",
      "epoch 2800: loss: 0.0018776286296083443\n",
      "epoch 2900: loss: 0.0017729139833101073\n",
      "epoch 3000: loss: 0.001678589981968529\n",
      "epoch 3100: loss: 0.001593226203551955\n",
      "epoch 3200: loss: 0.001515638765863554\n",
      "epoch 3300: loss: 0.0014448400367229725\n",
      "epoch 3400: loss: 0.0013800000623429615\n",
      "epoch 3500: loss: 0.0013204166695531408\n",
      "epoch 3600: loss: 0.0012654920639743323\n",
      "epoch 3700: loss: 0.00121471434610917\n",
      "epoch 3800: loss: 0.001167642788347132\n",
      "epoch 3900: loss: 0.0011238960151893497\n",
      "epoch 4000: loss: 0.0010831424443144655\n",
      "epoch 4100: loss: 0.0010450925027296126\n",
      "epoch 4200: loss: 0.0010094922473680482\n",
      "epoch 4300: loss: 0.0009761181049341427\n",
      "epoch 4400: loss: 0.0009447725097912348\n",
      "epoch 4500: loss: 0.0009152802670357291\n",
      "epoch 4600: loss: 0.0008874855047244572\n",
      "epoch 4700: loss: 0.0008612491074855128\n",
      "epoch 4800: loss: 0.0008364465455929591\n",
      "epoch 4900: loss: 0.0008129660305945439\n",
      "epoch 5000: loss: 0.0007907069419089536\n",
      "epoch 5100: loss: 0.0007695784793155059\n",
      "epoch 5200: loss: 0.0007494985045923803\n",
      "epoch 5300: loss: 0.0007303925422041412\n",
      "epoch 5400: loss: 0.0007121929142678978\n",
      "epoch 5500: loss: 0.0006948379893210804\n",
      "epoch 5600: loss: 0.0006782715278911939\n",
      "epoch 5700: loss: 0.0006624421106970666\n",
      "epoch 5800: loss: 0.0006473026376231416\n",
      "epoch 5900: loss: 0.0006328098875063065\n",
      "epoch 6000: loss: 0.0006189241303383064\n",
      "epoch 6100: loss: 0.0006056087847811045\n",
      "epoch 6200: loss: 0.0005928301149669035\n",
      "epoch 6300: loss: 0.0005805569614503648\n",
      "epoch 6400: loss: 0.0005687605019299188\n",
      "epoch 6500: loss: 0.0005574140379835581\n",
      "epoch 6600: loss: 0.0005464928045942851\n",
      "epoch 6700: loss: 0.000535973799687254\n",
      "epoch 6800: loss: 0.0005258356312797484\n",
      "epoch 6900: loss: 0.0005160583801667905\n",
      "epoch 7000: loss: 0.0005066234763395709\n",
      "epoch 7100: loss: 0.0004975135875680811\n",
      "epoch 7200: loss: 0.0004887125187801559\n",
      "epoch 7300: loss: 0.0004802051210413148\n",
      "epoch 7400: loss: 0.00047197720908835324\n",
      "epoch 7500: loss: 0.00046401548649752566\n",
      "epoch 7600: loss: 0.0004563074776790182\n",
      "epoch 7700: loss: 0.0004488414659853126\n",
      "epoch 7800: loss: 0.0004416064373045324\n",
      "epoch 7900: loss: 0.0004345920285823987\n",
      "epoch 8000: loss: 0.00042778848077985296\n",
      "epoch 8100: loss: 0.00042118659582861896\n",
      "epoch 8200: loss: 0.0004147776971956388\n",
      "epoch 8300: loss: 0.0004085535937097369\n",
      "epoch 8400: loss: 0.00040250654634133037\n",
      "epoch 8500: loss: 0.00039662923765887594\n",
      "epoch 8600: loss: 0.00039091474371485106\n",
      "epoch 8700: loss: 0.00038535650813972713\n",
      "epoch 8800: loss: 0.0003799483182450125\n",
      "epoch 8900: loss: 0.00037468428295678747\n",
      "epoch 9000: loss: 0.00036955881241876993\n",
      "epoch 9100: loss: 0.0003645665991200544\n",
      "epoch 9200: loss: 0.0003597026004167379\n",
      "epoch 9300: loss: 0.0003549620223292398\n",
      "epoch 9400: loss: 0.000350340304508469\n",
      "epoch 9500: loss: 0.0003458331062740269\n",
      "epoch 9600: loss: 0.00034143629363664264\n",
      "epoch 9700: loss: 0.0003371459272252027\n",
      "epoch 9800: loss: 0.00033295825104588596\n",
      "epoch 9900: loss: 0.000328869682007582\n",
      "[[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import itertools \n",
    "\n",
    "def generate_combination(n):\n",
    "    return np.array(list(itertools.product([0, 1], repeat=n)))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return (1/(1 + np.exp(-x)))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def forward_chaining(X, w1, b1, w2, b2, w3, b3):\n",
    "    z1 = np.dot(X, w1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "\n",
    "    z2 = np.dot(a1, w2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    z3 = np.dot(a2, w3) + b3\n",
    "    a3 = sigmoid(z3)\n",
    "\n",
    "    return a1, a2, a3\n",
    "\n",
    "num_inputs = int(input(\"Enter the number of binary inputs: \"))\n",
    "X = generate_combination(num_inputs)\n",
    "\n",
    "output = []\n",
    "for row in X:\n",
    "    print(f\"Enter the output for the inputs {list(row)}: \")\n",
    "    a = int(input())\n",
    "    output.append(a)\n",
    "\n",
    "y = np.array(output).reshape(-1, 1)\n",
    "n_hidden1 = 4\n",
    "n_hidden2 = 3\n",
    "output_size = 1 \n",
    "\n",
    "w1 = np.random.randn(num_inputs, n_hidden1)\n",
    "w2 = np.random.randn(n_hidden1, n_hidden2)\n",
    "w3 = np.random.randn(n_hidden2, output_size)\n",
    "\n",
    "b1 = np.zeros((1, n_hidden1))\n",
    "b2 = np.zeros((1, n_hidden2))\n",
    "b3 = np.zeros((1, output_size))\n",
    "\n",
    "epoch = 5000\n",
    "learning_rate = 0.1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    a1, a2, result = forward_chaining(X, w1, b1, w2, b2, w3, b3)\n",
    "\n",
    "    loss = np.mean((y - result) ** 2)\n",
    "\n",
    "    error = y - result \n",
    "    d_output = error * sigmoid_derivative(result) \n",
    "\n",
    "    error2 = d_output.dot(w3.T)\n",
    "    d_hidden2 = error2 * sigmoid_derivative(a2)\n",
    "\n",
    "    error1 = d_hidden2.dot(w2.T)\n",
    "    d_hidden1 = error1 * sigmoid_derivative(a1) \n",
    "\n",
    "\n",
    "    w3 +=  a2.T.dot(d_output) * learning_rate \n",
    "    b3 +=  np.sum(d_output, axis=0, keepdims=True) * learning_rate \n",
    "    w2 +=  a1.T.dot(d_hidden2) * learning_rate \n",
    "    b2 +=  np.sum(d_hidden2, axis=0, keepdims=True) * learning_rate \n",
    "    w1 +=  X.T.dot(d_hidden1) * learning_rate \n",
    "    b1 +=  np.sum(d_hidden1, axis=0, keepdims=True) * learning_rate \n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"epoch {epoch}: loss: {loss}\")\n",
    "a1, a2, a3 = forward_chaining(X, w1, b1, w2, b2, w3, b3)\n",
    "print(np.round(a3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
